import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import h5py
import numpy as np
import os
import torch.jit

# ====================================================================
# A. æ¨¡å‹ç»“æ„: èåˆäº† Positional Encoding å’Œ Skip Connections çš„ RNDF é£æ ¼ MLP
#    - ä¿®æ”¹ç‚¹ï¼šè¾“å‡ºç»´åº¦å˜ä¸º 2 (SDF, Classification Logit)
# ====================================================================
class RNDFMLP(nn.Module):
    def __init__(self, input_dim=6, hidden_dim=256, num_layers=5, skip_layer=2):
        super().__init__()
        
        # 1. Positional Encoding (Sin/Cos Feature Map)
        self.input_layer = nn.Linear(input_dim, hidden_dim) 
        self.pos_encoding_layer = nn.Linear(input_dim * 2, hidden_dim)
        
        # 2. ç½‘ç»œä¸»ä½“ (Body) - ä¿æŒä¸å˜
        layers = []
        current_dim = hidden_dim
        
        for i in range(num_layers):
            if i == skip_layer:
                layers.append(nn.Linear(current_dim + hidden_dim, hidden_dim)) 
            else:
                layers.append(nn.Linear(current_dim, hidden_dim))
            
            layers.append(nn.LeakyReLU(0.1)) 
            current_dim = hidden_dim

        self.hidden_layers = nn.Sequential(*layers)
        
        # 3. è¾“å‡ºå±‚ - æ‹†åˆ†æˆä¸¤ä¸ªç‹¬ç«‹çš„å¤´
        self.sdf_output_layer = nn.Linear(hidden_dim, 1)      # SDF å€¼ (å›å½’)
        self.class_output_layer = nn.Linear(hidden_dim, 1)    # åˆ†ç±» Logit (äºŒåˆ†ç±»)

        self.num_layers = num_layers
        self.skip_layer = skip_layer
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')
                nn.init.constant_(m.bias, 0.0)

    def forward(self, q):
        # q: [batch_size, 6] å…³èŠ‚é…ç½®
        
        # 1. Positional Encoding: q_in = [sin(q), cos(q)]
        q_pos_encoded = torch.cat([torch.sin(q), torch.cos(q)], dim=-1)
        
        # x_skip: è·³è·ƒè¿æ¥æºå¸¦çš„ç‰¹å¾ï¼Œç»´åº¦ [batch_size, hidden_dim] (256)
        x_skip = self.pos_encoding_layer(q_pos_encoded)
        x = x_skip # åˆå§‹åŒ–éšè—å±‚è¾“å…¥ [batch_size, 256]

        # 2. éšè—å±‚å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.hidden_layers):
            if i == self.skip_layer * 2: 
                x = torch.cat([x, x_skip], dim=-1)
            x = layer(x)
        
        # 3. åŒå¤´è¾“å‡º
        # pred_sdf: [batch_size] SDF å›å½’é¢„æµ‹
        pred_sdf = self.sdf_output_layer(x).squeeze(-1)
        # pred_logit: [batch_size] ç¢°æ’åˆ†ç±» Logit é¢„æµ‹
        pred_logit = self.class_output_layer(x).squeeze(-1)

        return pred_sdf, pred_logit # è¿”å›ä¸¤ä¸ªå€¼


# ====================================================================
# B. æŸå¤±å‡½æ•°
#    - ä¿®æ”¹ç‚¹ï¼šæ–°å¢ BCEWithLogitsLossï¼Œå¹¶åˆ›å»ºæ··åˆæŸå¤±å‡½æ•°
# ====================================================================
def weighted_mse_loss(pred, target, beta=5.0): 
    # å›å½’æŸå¤±ï¼Œæ›´å…³æ³¨æ¥è¿‘ SDF=0 çš„ç‚¹
    weight = torch.exp(-beta * torch.abs(target))
    return (weight * (pred - target) ** 2).mean()

# ä½¿ç”¨ PyTorch å†…ç½®çš„ BCEWithLogitsLossï¼Œå®ƒæ›´ç¨³å®š
bce_loss_fn = nn.BCEWithLogitsLoss()

def mixed_loss(pred_sdf, pred_logit, target_sdf, target_class, lambda_bce=1.0, beta_mse=5.0):
    # 1. å›å½’æŸå¤± (Weighted MSE)
    loss_sdf = weighted_mse_loss(pred_sdf, target_sdf, beta=beta_mse)
    
    # 2. åˆ†ç±»æŸå¤± (BCE)
    # ç›®æ ‡ç±»åˆ« (0.0=å®‰å…¨/è¾¹ç•Œ, 1.0=ç¢°æ’)
    loss_bce = bce_loss_fn(pred_logit, target_class)
    
    # 3. æ··åˆæŸå¤±
    total_loss = loss_sdf + lambda_bce * loss_bce
    return total_loss, loss_sdf, loss_bce


# ====================================================================
# C. è¯„ä¼°å‡½æ•° (ä¿æŒä¸å˜ï¼Œåªä½¿ç”¨ SDF é¢„æµ‹è¿›è¡Œè¯„ä¼°)
# ====================================================================
def evaluate_metrics(pred, target, threshold=0.05):
    pred = np.array(pred)
    target = np.array(target)
    # Collision recall: true collision (target<0) detected as risky (pred<threshold)
    true_collision = target < 0
    pred_risky = pred < threshold
    if true_collision.sum() == 0:
        recall = 1.0
    else:
        recall = np.mean(pred_risky[true_collision])
    # False positive rate
    true_safe = target > 0.2
    fp = np.mean(pred_risky[true_safe]) if true_safe.sum() > 0 else 0.0
    # MAE near boundary
    near_boundary = (target >= -0.05) & (target <= 0.05)
    mae_boundary = np.mean(np.abs(pred[near_boundary] - target[near_boundary])) if near_boundary.any() else 0.0
    return recall, fp, mae_boundary

# ====================================================================
# D. ä¸»è®­ç»ƒå‡½æ•°
#    - ä¿®æ”¹ç‚¹ï¼šå‡†å¤‡åˆ†ç±»æ ‡ç­¾ï¼Œä½¿ç”¨æ··åˆæŸå¤±
# ====================================================================
def main():
    # è·¯å¾„ (å‡è®¾æ‚¨å·²ç”Ÿæˆ sdf_dataset_train.h5)
    train_hdf5_path = "dataset/sdf_dataset_train.h5" 
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶åŠ è½½
    if not os.path.exists(train_hdf5_path):
        print(f"Error: Training dataset not found at {train_hdf5_path}. Please run data generation first.")
        return

    with h5py.File(train_hdf5_path, "r") as f:
        X = f["joint_configs"][:]
        y_sdf = f["sdf_values"][:].flatten()

    print(f"Loaded {len(X)} training samples.")

    # **æ–°å¢ï¼šæ ¹æ® SDF å€¼ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾**
    # ç¢°æ’ (SDF <= 0) = 1.0, å®‰å…¨/è¾¹ç•Œ (SDF > 0) = 0.0
    y_class = (y_sdf <= 0).astype(np.float32)

    # æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›† (ç°åœ¨æ‹†åˆ† X, y_sdf, y_class)
    X_train, X_val, y_sdf_train, y_sdf_val, y_class_train, y_class_val = train_test_split(
        X, y_sdf, y_class, test_size=0.1, random_state=42
    )

    # è½¬æ¢ä¸º Tensor å’Œ DataLoader
    train_loader = DataLoader(
        TensorDataset(
            torch.from_numpy(X_train).float(), 
            torch.from_numpy(y_sdf_train).float(),
            torch.from_numpy(y_class_train).float() # æ–°å¢åˆ†ç±»æ ‡ç­¾
        ),
        batch_size=512, shuffle=True 
    )
    X_val_t = torch.from_numpy(X_val).float()
    y_sdf_val_t = torch.from_numpy(y_sdf_val).float()
    
    # === æ¨¡å‹å’Œä¼˜åŒ–å™¨ ===
    model = RNDFMLP(input_dim=6, hidden_dim=256, num_layers=5, skip_layer=2)
    optimizer = optim.Adam(model.parameters(), lr=5e-4) 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # æŸå¤±æƒé‡å‚æ•°
    LAMBDA_BCE = 1.0 # äº¤å‰ç†µæŸå¤±æƒé‡
    BETA_MSE = 5.0   # MSE æŸå¤±çš„ beta å‚æ•°

    best_recall = 0.0
    patience = 20 
    no_improve = 0
    max_epochs = 200

    print(f"ğŸš€ Training started with RNDFMLP (Mixed Loss: SDF + {LAMBDA_BCE}*BCE)...")
    for epoch in range(max_epochs):
        model.train()
        epoch_loss = 0.0
        epoch_loss_sdf = 0.0
        epoch_loss_bce = 0.0
        
        # å­¦ä¹ ç‡è°ƒåº¦ 
        if epoch == max_epochs // 2:
             for param_group in optimizer.param_groups:
                 param_group['lr'] *= 0.1
                 print(f"--- Learning rate decayed to {param_group['lr']} ---")

        # è¿­ä»£å™¨ç°åœ¨è¾“å‡º x_batch, y_sdf_batch, y_class_batch
        for x_batch, y_sdf_batch, y_class_batch in train_loader:
            x_batch = x_batch.to(device)
            y_sdf_batch = y_sdf_batch.to(device)
            y_class_batch = y_class_batch.to(device) # æ–°å¢

            optimizer.zero_grad()
            
            # æ¨¡å‹ç°åœ¨è¿”å›ä¸¤ä¸ªè¾“å‡º
            pred_sdf, pred_logit = model(x_batch)
            
            # è®¡ç®—æ··åˆæŸå¤±
            loss, loss_sdf, loss_bce = mixed_loss(
                pred_sdf, pred_logit, y_sdf_batch, y_class_batch, 
                lambda_bce=LAMBDA_BCE, beta_mse=BETA_MSE
            )
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_loss_sdf += loss_sdf.item()
            epoch_loss_bce += loss_bce.item()

        # Validation
        model.eval()
        with torch.no_grad():
            # éªŒè¯æ—¶åªä½¿ç”¨ SDF é¢„æµ‹ (ç¬¬ä¸€ä¸ªè¾“å‡º)
            val_pred_sdf, _ = model(X_val_t.to(device))
            val_pred = val_pred_sdf.cpu().numpy()

        recall, fp, mae_boundary = evaluate_metrics(val_pred, y_sdf_val)

        print(f"Epoch {epoch+1:3d} | Total Loss: {epoch_loss/len(train_loader):.4f} "
              f"(SDF: {epoch_loss_sdf/len(train_loader):.4f}, BCE: {epoch_loss_bce/len(train_loader):.4f}) | "
              f"Recall: {recall:.3f} | FP: {fp:.3f} | MAE@0: {mae_boundary:.4f} | Best Recall: {best_recall:.3f}")

        # Save best model by recall
        if recall > best_recall:
            best_recall = recall
            no_improve = 0
            os.makedirs("models", exist_ok=True)
            # ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼Œå› ä¸ºç»“æ„å‘ç”Ÿäº†å˜åŒ–
            torch.save(model.state_dict(), "models/best_sdf_class_model.pth")
        else:
            no_improve += 1
            if no_improve >= patience:
                print("Early stopping!")
                break

    # === åŠ è½½æœ€ä½³æ¨¡å‹å¹¶å¯¼å‡º ===
    model.load_state_dict(torch.load("models/best_sdf_class_model.pth", map_location=device))
    model.eval()

    # TorchScript export (åªå¯¼å‡º SDF é¢„æµ‹éƒ¨åˆ†)
    class SDFPredictor(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
        
        def forward(self, q):
            # åªéœ€è¦ SDF é¢„æµ‹
            pred_sdf, _ = self.model(q)
            return pred_sdf

    sdf_predictor = SDFPredictor(model.cpu()) # å…ˆå°†æ¨¡å‹ç§»åˆ° CPU è¿›è¡Œ tracing
    example_input = torch.randn(1, 6)
    traced_model = torch.jit.trace(sdf_predictor, example_input) 
    traced_model.save("models/fast_sdf_model_mixed.pt")
    print("âœ… SDF Predictor Model exported to models/fast_sdf_model_mixed.pt")

    # === Final evaluation ===
    model.to(device)
    # åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåˆ›å»ºæœ€ç»ˆçš„ Tensor
    X_full_t = torch.from_numpy(X).float().to(device)
    y_full_sdf = y_sdf
    
    # Final metrics 
    with torch.no_grad():
        final_pred_sdf, _ = model(X_full_t)
        final_pred = final_pred_sdf.cpu().detach().numpy()
        
    recall, fp, mae_boundary = evaluate_metrics(final_pred, y_full_sdf)
    print("\nğŸ“Š Final Metrics on Full Training Dataset (Best Model):")
    print(f"   Collision Recall (threshold=0.05): {recall:.4f}")
    print(f"   False Positive Rate (safe>0.2):   {fp:.4f}")
    print(f"   MAE near SDF=0:                  {mae_boundary:.4f}")

if __name__ == "__main__":
    main()
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import h5py
import numpy as np
import os
import torch.jit

# ====================================================================
# A. æ¨¡å‹ç»“æ„: èåˆäº† Positional Encoding å’Œ Skip Connections çš„ RNDF é£æ ¼ MLP
# ====================================================================
class RNDFMLP(nn.Module):
    def __init__(self, input_dim=6, hidden_dim=256, num_layers=5, skip_layer=2):
        super().__init__()
        
        # 1. Positional Encoding (Sin/Cos Feature Map)
        self.input_layer = nn.Linear(input_dim, hidden_dim) 
        self.pos_encoding_layer = nn.Linear(input_dim * 2, hidden_dim)
        
        # 2. ç½‘ç»œä¸»ä½“ (Body)
        layers = []
        current_dim = hidden_dim
        
        for i in range(num_layers):
            # å¤„ç†è·³è·ƒè¿æ¥çš„æ‹¼æ¥è¾“å…¥: è¾“å…¥ç»´åº¦å˜ä¸º (å½“å‰ç»´åº¦ + è·³è·ƒè¿æ¥ç»´åº¦)
            if i == skip_layer:
                layers.append(nn.Linear(current_dim + hidden_dim, hidden_dim)) 
            else:
                layers.append(nn.Linear(current_dim, hidden_dim))
            
            layers.append(nn.LeakyReLU(0.1)) 
            current_dim = hidden_dim

        self.hidden_layers = nn.Sequential(*layers)
        
        # 3. è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_dim, 1)

        self.num_layers = num_layers
        self.skip_layer = skip_layer
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')
                nn.init.constant_(m.bias, 0.0)

    def forward(self, q):
        # q: [batch_size, 6] å…³èŠ‚é…ç½®
        
        # 1. Positional Encoding: q_in = [sin(q), cos(q)]
        q_pos_encoded = torch.cat([torch.sin(q), torch.cos(q)], dim=-1)
        
        # x_skip: è·³è·ƒè¿æ¥æºå¸¦çš„ç‰¹å¾ï¼Œç»´åº¦ [batch_size, hidden_dim] (256)
        x_skip = self.pos_encoding_layer(q_pos_encoded)
        x = x_skip # åˆå§‹åŒ–éšè—å±‚è¾“å…¥ [batch_size, 256]

        # 2. éšè—å±‚å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.hidden_layers):
            # ä¿®å¤ç‚¹ï¼šä»…åœ¨ç›®æ ‡çº¿æ€§å±‚ (å¶æ•°ç´¢å¼•) ä¹‹å‰è¿›è¡Œæ‹¼æ¥
            if i == self.skip_layer * 2: 
                x = torch.cat([x, x_skip], dim=-1)
            x = layer(x)

        return self.output_layer(x).squeeze(-1)


# ====================================================================
# B. æŸå¤±å‡½æ•°
# ====================================================================
# def collision_type_loss(pred, target):
#     Bceloss = nn.BCEWithLogitsLoss()

def weighted_mse_loss(pred, target, beta=5.0): 
    weight = torch.exp(-beta * torch.abs(target))
    return (weight * (pred - target) ** 2).mean()

# ====================================================================
# C. è¯„ä¼°å‡½æ•°
# ====================================================================
def evaluate_metrics(pred, target, threshold=0.05):
    pred = np.array(pred)
    target = np.array(target)
    # Collision recall: true collision (target<0) detected as risky (pred<threshold)
    true_collision = target < 0
    pred_risky = pred < threshold
    if true_collision.sum() == 0:
        recall = 1.0
    else:
        recall = np.mean(pred_risky[true_collision])
    # False positive rate
    true_safe = target > 0.2
    fp = np.mean(pred_risky[true_safe]) if true_safe.sum() > 0 else 0.0
    # MAE near boundary
    near_boundary = (target >= -0.05) & (target <= 0.05)
    mae_boundary = np.mean(np.abs(pred[near_boundary] - target[near_boundary])) if near_boundary.any() else 0.0
    return recall, fp, mae_boundary

# ====================================================================
# D. ä¸»è®­ç»ƒå‡½æ•°
# ====================================================================
def main():
    # è·¯å¾„ (å‡è®¾æ‚¨å·²ç”Ÿæˆ sdf_dataset_train.h5)
    train_hdf5_path = "dataset/sdf_dataset_train.h5" 
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶åŠ è½½
    if not os.path.exists(train_hdf5_path):
        print(f"Error: Training dataset not found at {train_hdf5_path}. Please run data generation first.")
        return

    with h5py.File(train_hdf5_path, "r") as f:
        X = f["joint_configs"][:]
        y = f["sdf_values"][:].flatten()

    print(f"Loaded {len(X)} training samples.")

    # æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›† (ä½¿ç”¨æ•´ä¸ªæ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ï¼Œç„¶åå†…éƒ¨æ‹†åˆ†)
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

    # è½¬æ¢ä¸º Tensor å’Œ DataLoader
    train_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()),
        batch_size=512, shuffle=True 
    )
    X_val_t = torch.from_numpy(X_val).float()
    y_val_t = torch.from_numpy(y_val).float()
    
    # === æ¨¡å‹å’Œä¼˜åŒ–å™¨ (ä½¿ç”¨æ–°çš„ RNDFMLP) ===
    # å¢åŠ æ¨¡å‹å®¹é‡: hidden_dim=256, num_layers=5
    model = RNDFMLP(input_dim=6, hidden_dim=256, num_layers=5, skip_layer=2)
    optimizer = optim.Adam(model.parameters(), lr=5e-4) 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    best_recall = 0.0
    patience = 20 
    no_improve = 0
    max_epochs = 200

    print("ğŸš€ Training started with RNDFMLP...")
    for epoch in range(max_epochs):
        model.train()
        epoch_loss = 0.0
        
        # å­¦ä¹ ç‡è°ƒåº¦ 
        if epoch == max_epochs // 2:
             for param_group in optimizer.param_groups:
                 param_group['lr'] *= 0.1
                 print(f"--- Learning rate decayed to {param_group['lr']} ---")

        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            pred = model(x_batch)
            loss = weighted_mse_loss(pred, y_batch, beta=5.0) 
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # Validation
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_t.to(device)).cpu().numpy()
        recall, fp, mae_boundary = evaluate_metrics(val_pred, y_val)

        print(f"Epoch {epoch+1:3d} | Loss: {epoch_loss/len(train_loader):.4f} | "
              f"Recall: {recall:.3f} | FP: {fp:.3f} | MAE@0: {mae_boundary:.4f} | Best Recall: {best_recall:.3f}")

        # Save best model by recall
        if recall > best_recall:
            best_recall = recall
            no_improve = 0
            os.makedirs("models", exist_ok=True)
            torch.save(model.state_dict(), "models/best_sdf_model.pth")
        else:
            no_improve += 1
            if no_improve >= patience:
                print("Early stopping!")
                break

    # === åŠ è½½æœ€ä½³æ¨¡å‹å¹¶å¯¼å‡º ===
    model.load_state_dict(torch.load("models/best_sdf_model.pth", map_location=device))
    model.eval()

    # TorchScript export
    example_input = torch.randn(1, 6).to(device)
    traced_model = torch.jit.trace(model.cpu(), example_input.cpu()) 
    traced_model.save("models/fast_sdf_model.pt")
    print("âœ… Model exported to models/fast_sdf_model.pt")

    # === FIX: Move model back to the GPU for final evaluation ===
    model.to(device) 

    # Final metrics (åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸ŠæŠ¥å‘Šæœ€ç»ˆæŒ‡æ ‡)
    final_pred = model(torch.from_numpy(X).float().to(device)).cpu().detach().numpy()
    recall, fp, mae_boundary = evaluate_metrics(final_pred, y)
    print("\nğŸ“Š Final Metrics on Full Training Dataset (Best Model):")
    print(f"   Collision Recall (threshold=0.05): {recall:.4f}")
    print(f"   False Positive Rate (safe>0.2):   {fp:.4f}")
    print(f"   MAE near SDF=0:                  {mae_boundary:.4f}")

if __name__ == "__main__":
    main()